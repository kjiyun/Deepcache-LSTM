{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvwbKocxByzc1awPrQWC7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjiyun/Deepcache-LSTM/blob/main/Deepcache_Dataset2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql6Y4hVS-RlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ec88cf-685e-4c6b-999f-3d3df6b114f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:170: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:176: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:183: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:189: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:195: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:200: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:218: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:314: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:316: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:170: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:176: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:183: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:189: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:195: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:200: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:218: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:314: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:316: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating hourly request ratio file ...\n",
            "Generating Objects for Dataset ...\n",
            "Generating Requests for Dataset ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-ac6a595eb956>:170: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if dist is 'pareto':\n",
            "<ipython-input-1-ac6a595eb956>:176: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if dist is 'paretoScaled':\n",
            "<ipython-input-1-ac6a595eb956>:183: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif dist is 'normal':\n",
            "<ipython-input-1-ac6a595eb956>:189: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif dist is 'logNormal':\n",
            "<ipython-input-1-ac6a595eb956>:195: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif dist is 'exp':\n",
            "<ipython-input-1-ac6a595eb956>:200: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif dist is 'poisson':\n",
            "<ipython-input-1-ac6a595eb956>:218: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if typeMode is 'HPC':\n",
            "<ipython-input-1-ac6a595eb956>:314: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if objMode[i] is 'regular':\n",
            "<ipython-input-1-ac6a595eb956>:316: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif objMode[i] is 'news':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 Days Processed of 212 Total Days\n",
            "40 Days Processed of 212 Total Days\n",
            "60 Days Processed of 212 Total Days\n",
            "80 Days Processed of 212 Total Days\n",
            "100 Days Processed of 212 Total Days\n",
            "120 Days Processed of 212 Total Days\n",
            "140 Days Processed of 212 Total Days\n",
            "160 Days Processed of 212 Total Days\n",
            "180 Days Processed of 212 Total Days\n",
            "200 Days Processed of 212 Total Days\n",
            "MediSyn Dataset Saved to Output file: ./Datasets/mediSynDataset_x2_O1034.csv\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import csv\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "\"\"\"\"###########################  initialization  ####################################################################\"\"\"\n",
        "NUM_OF_OBJECTS = 1000                            # number of objects to generate\n",
        "lambdaFile = 'hourly_request_ratio.csv'         # file with hourly request ratio\n",
        "lambdas = []                                    # stores the diurnal ratios for non-homogeneous Poisson\n",
        "curTime = []                                    # for each object shows the last timeStamp that it was requested\n",
        "objectPopularities = []                         # Contains object popularity\n",
        "M = 178310                                      # max frequency for HPC dataset\n",
        "traceType = 'HPC'\n",
        "hourly_request_function_degree = 2              # the degree for the function that sets the objects per bin pattern, X^2\n",
        "dayGaps = []                                    # interarrival between days\n",
        "numOfObjectsIntroduced = []                     # number of objects generated in each day\n",
        "interArrivals = []                              # generates the interarrival time between objects introduced in a day\n",
        "lifeSpanType = []                               # for each object it holds the type of its lifeSpan\n",
        "ObjectsLifeSpan = []                            # the length of lifeSpan value for each object\n",
        "requestGenInfo = {}                             # for each object it holds the info about requests\n",
        "startTimes = {}                                 # sorted objects based on their introduction time\n",
        "introductionOrder = []                          # random order for introducing objects in a day\n",
        "sortedOnIntoTime = []\n",
        "requests = []                                   # generated requests\n",
        "objectLengths = []\n",
        "if sys.version_info[0] < 3:\n",
        "    maxEndDay = -sys.maxint - 1\n",
        "else:\n",
        "    maxEndDay = -sys.maxsize - 1\n",
        "WITH_INTRODUCTION = True                        # flag to allow objects to be introduced at any time\n",
        "WITH_DAY_GAPS_INTRODUCTION = False              # If True, introduce gaps between the objects introduction days,\n",
        "                                                # otherwise objects are introduced each day\n",
        "GENERATE_NEW_HOURLY_REQUEST_RATIO = False       # If True, a new 'hourly_request_ratio.csv' is generated\n",
        "MIN_REQ_PER_DAY_THRESHOLD = 1500                # min number of requests to be generated for each object in a day\n",
        "MIN_OBJ_INTRODCUED_PER_DAY_THRESHOLD = 0.0035 * NUM_OF_OBJECTS  # min number of objects to be generated in a day\n",
        "MAX_OBJ_INTRODCUED_PER_DAY_THRESHOLD = 0.0095 * NUM_OF_OBJECTS  # max number of objects to be generated in a day\n",
        "\n",
        "# Creating output directory if it does not exist\n",
        "OUTPUTDIR = './Datasets'\n",
        "if not os.path.isdir(OUTPUTDIR):\n",
        "    os.makedirs(OUTPUTDIR)\n",
        "\n",
        "# Checking the existence of hourly_request_ratio.csv file\n",
        "if not os.path.isfile('hourly_request_ratio.csv'):\n",
        "    GENERATE_NEW_HOURLY_REQUEST_RATIO = True\n",
        "\n",
        "if GENERATE_NEW_HOURLY_REQUEST_RATIO:\n",
        "    print('Generating hourly request ratio file ...')\n",
        "    rands = np.random.randint(1, 100, 24)\n",
        "    rands = rands/float(np.sum(rands))\n",
        "    index = np.arange(1, 25)\n",
        "\n",
        "    res = 'hourly_request_ratio.csv'\n",
        "    f = open(res, 'w+')\n",
        "    for i in range(len(index)):\n",
        "        if i != len(index)-1:\n",
        "            f.write(str(index[i]) + ',' + str(rands[i])+'\\n')\n",
        "        else:\n",
        "            f.write(str(index[i]) + ',' + str(rands[i]))\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def initialize():\n",
        "    global curTime\n",
        "    loadDiurnalRatios()\n",
        "    print('Generating Objects for Dataset ...')\n",
        "    generateObjectsIntroductionInfo(traceType)\n",
        "    generatePopularities(traceType, int(NUM_OF_OBJECTS))\n",
        "    generateObjects()\n",
        "\n",
        "    print('Generating Requests for Dataset ...')\n",
        "    curTime = [0] * NUM_OF_OBJECTS\n",
        "    generateRequests()\n",
        "\n",
        "\n",
        "\"\"\"################################ Load diurnal ratios #############################################################\"\"\"\n",
        "def loadDiurnalRatios():\n",
        "    with open(lambdaFile, \"r+\") as fi:\n",
        "        for line in fi:\n",
        "            tmpLambdas = float(line.rstrip('\\n').rstrip('\\r').split(',')[1])\n",
        "            lambdas.append(tmpLambdas)\n",
        "    fi.close()\n",
        "\n",
        "\n",
        "\"\"\"###########################  Object Popularity  ##################################################################\"\"\"\n",
        "K = {'HPC': 30, 'HCL': 7}\n",
        "\n",
        "\n",
        "def generatePopularities(traceType, N):\n",
        "    zipalpha = 0.8\n",
        "    k = K[traceType]\n",
        "    for i in range(1, N+1):\n",
        "        Mk = ((M-1)/k)+1\n",
        "        tmp = (((float(Mk)/(math.pow((float(i+k-1)/k), zipalpha)))-1)*k)+1\n",
        "        objectPopularities.append(tmp)\n",
        "\n",
        "\n",
        "\"\"\"########################  Object Type  ###########################################################################\"\"\"\n",
        "def getObjectType():\n",
        "    decision = random.uniform(0, 1)\n",
        "    if decision <= 0.1:  # 10 % of objects are news\n",
        "        return 'news'\n",
        "    else:\n",
        "        return 'regular'\n",
        "\n",
        "\n",
        "\"\"\"##################### generating random variates #################################################################\"\"\"\n",
        "def generatePoissonVariate(rand, lambda_poisson):\n",
        "    \"\"\"\n",
        "    for diurnal access generation\n",
        "    \"\"\"\n",
        "    return -1 * (math.log(1-rand))/lambda_poisson\n",
        "\n",
        "\n",
        "def generateParetoVariate(rand, alpha):\n",
        "    return math.pow(1/rand, 1/alpha)\n",
        "\n",
        "\n",
        "def generateParetoScaledVariate(rand, alpha, beta):\n",
        "    \"\"\" F(x) = 1 - (b/x)^a, x >= b \"\"\"\n",
        "    return beta / (math.pow((1 - rand), (1/alpha)))\n",
        "\n",
        "\n",
        "def generateNormalVariate(mu, sigma):\n",
        "    \"\"\"\n",
        "    RV generated using rejection method\n",
        "    \"\"\"\n",
        "    variateGenerated = False\n",
        "    while not variateGenerated:\n",
        "        u1 = random.uniform(0, 1)\n",
        "        u2 = random.uniform(0, 1)\n",
        "        x = -1*math.log(u1)\n",
        "        if u2 > math.exp(-1*math.pow((x-1), 2)/2):\n",
        "            continue\n",
        "        else:\n",
        "            u3 = random.uniform(0, 1)\n",
        "            if u3 > 0.5:\n",
        "                return mu+(sigma*x)\n",
        "            else:\n",
        "                return mu-(sigma*x)\n",
        "\n",
        "\n",
        "def generateLogNormalVariate(mu, sigma):\n",
        "    \"\"\"\n",
        "    RV generated using rejection method\n",
        "    \"\"\"\n",
        "    variateGenerated = False\n",
        "    while not variateGenerated:\n",
        "        u1 = random.uniform(0, 1)\n",
        "        u2 = random.uniform(0, 1)\n",
        "        x = -1*math.log(u1)\n",
        "        if u2 > math.exp(-1*math.pow((x-1), 2)/2):\n",
        "            continue\n",
        "        else:\n",
        "            return math.exp(mu+(sigma*x))\n",
        "\n",
        "\n",
        "def generateExponentialVariate(rand, a):\n",
        "    return -(1/a)*math.log(1-rand)\n",
        "\n",
        "\n",
        "def generateRandVariate(dist, params, numOfVariates):\n",
        "    variates = []\n",
        "\n",
        "    if dist is 'pareto':\n",
        "        alpha = params['alpha']\n",
        "        for i in range(numOfVariates):\n",
        "            rand = random.uniform(0, 1)\n",
        "            variates.append(generateParetoVariate(rand, alpha))\n",
        "\n",
        "    if dist is 'paretoScaled':\n",
        "        alpha = params['alpha']\n",
        "        beta = params['beta']\n",
        "        for i in range(numOfVariates):\n",
        "            rand = random.uniform(0, 1)\n",
        "            variates.append(generateParetoScaledVariate(rand, alpha, beta))\n",
        "\n",
        "    elif dist is 'normal':\n",
        "        mu = params['mu']\n",
        "        sigma = params['sigma']\n",
        "        for i in range(numOfVariates):\n",
        "            variates.append(generateNormalVariate(mu, sigma))\n",
        "\n",
        "    elif dist is 'logNormal':\n",
        "        mu = params['mu']\n",
        "        sigma = params['sigma']\n",
        "        for i in range(numOfVariates):\n",
        "            variates.append(generateLogNormalVariate(mu, sigma))\n",
        "\n",
        "    elif dist is 'exp':\n",
        "        mu = params['mu']\n",
        "        for i in range(numOfVariates):\n",
        "            rand = random.uniform(0, 1)\n",
        "            variates.append(generateExponentialVariate(rand, mu))\n",
        "    elif dist is 'poisson':\n",
        "        mu = params['mu']\n",
        "        for i in range(numOfVariates):\n",
        "            rand = random.uniform(0, 1)\n",
        "            variates.append(generatePoissonVariate(rand, mu))\n",
        "    return variates\n",
        "\n",
        "\n",
        "\"\"\"####################  Object Introduction Info  ##################################################################\"\"\"\n",
        "def generateObjectsIntroductionInfo(typeMode):\n",
        "    \"\"\"\n",
        "    generates gaps between introduction days based on either pareto or exponential distribution\n",
        "    \"\"\"\n",
        "    global NUM_OF_OBJECTS\n",
        "    global numOfObjectsIntroduced\n",
        "\n",
        "    tempNumOfObjectsIntroduced = []\n",
        "    while sum(tempNumOfObjectsIntroduced) < NUM_OF_OBJECTS:\n",
        "        if typeMode is 'HPC':\n",
        "            if WITH_DAY_GAPS_INTRODUCTION:\n",
        "                pareto_alpha_objectIntro_hpc = 1.0164\n",
        "                object_intro_days_gap = generateRandVariate('pareto', {'alpha':pareto_alpha_objectIntro_hpc}, 1)[0]\n",
        "                if object_intro_days_gap > 20:\n",
        "                    object_intro_days_gap = 20\n",
        "                dayGaps.append(object_intro_days_gap)\n",
        "            else:\n",
        "                dayGaps.append(1)\n",
        "\n",
        "        else:\n",
        "            exponential_mu_objectIntro_hpl = 4.2705\n",
        "            object_intro_days_gap = generateRandVariate('exp', {'mu': exponential_mu_objectIntro_hpl}, 1)[0]\n",
        "            dayGaps.append(object_intro_days_gap)\n",
        "\n",
        "        # number of new objects generated in each introduction day Pareto dist\n",
        "        pareto_alpha_numOfObjectsGeneration = 0.8\n",
        "        pareto_beta_numOfObjectsGeneration = MIN_OBJ_INTRODCUED_PER_DAY_THRESHOLD\n",
        "        numOfObjects_intro_in_day = generateRandVariate('paretoScaled', {'alpha': pareto_alpha_numOfObjectsGeneration,\n",
        "                                                        'beta': pareto_beta_numOfObjectsGeneration}, 1)[0]\n",
        "        if numOfObjects_intro_in_day > MAX_OBJ_INTRODCUED_PER_DAY_THRESHOLD:\n",
        "            numOfObjects_intro_in_day = MAX_OBJ_INTRODCUED_PER_DAY_THRESHOLD\n",
        "        tempNumOfObjectsIntroduced.append(numOfObjects_intro_in_day)\n",
        "\n",
        "    # sort generated items\n",
        "    tempNumOfObjectsIntroduced.sort()\n",
        "    extra_days = 0\n",
        "    if len(tempNumOfObjectsIntroduced) % 7 != 0:\n",
        "        extra_days = len(tempNumOfObjectsIntroduced) % 7\n",
        "        for i in range(extra_days):\n",
        "            # generate random int to add these objects to other introduction days to generate full weeks of data\n",
        "            added = False\n",
        "            while not added:\n",
        "                u = random.randint(extra_days+1, len(tempNumOfObjectsIntroduced) - 1)\n",
        "                if tempNumOfObjectsIntroduced[i] + tempNumOfObjectsIntroduced[u] < MAX_OBJ_INTRODCUED_PER_DAY_THRESHOLD:\n",
        "                    tempNumOfObjectsIntroduced[u] += tempNumOfObjectsIntroduced[i]\n",
        "                    added = True\n",
        "\n",
        "    # Exclude the extra days after being added to other days\n",
        "    tempNumOfObjectsIntroduced = tempNumOfObjectsIntroduced[extra_days:]\n",
        "    tempNumOfObjectsIntroduced.sort()\n",
        "\n",
        "    # Fill in the days by dividing the sorted data as following\n",
        "    # This induces that more objects are introduced on Friday then Saturday, and so on.\n",
        "    # The least number of objects are introduced on Tuesday.\n",
        "    # Fri 1, Sat 2, Sun 3, Thu 4, Wed 5, Mon 6, Tuesday 7\n",
        "    weeks = int(len(tempNumOfObjectsIntroduced) / 7)\n",
        "    FriIndex = weeks * 6\n",
        "    SatIndex = weeks * 5\n",
        "    SunIndex = weeks * 4\n",
        "    MonIndex = weeks * 1\n",
        "    TuesIndex = weeks * 0\n",
        "    WedIndex = weeks * 2\n",
        "    ThuIndex = weeks * 3\n",
        "\n",
        "    for i in range(weeks):\n",
        "        numOfObjectsIntroduced.append(tempNumOfObjectsIntroduced[MonIndex+i])\n",
        "        numOfObjectsIntroduced.append(tempNumOfObjectsIntroduced[TuesIndex + i])\n",
        "        numOfObjectsIntroduced.append(tempNumOfObjectsIntroduced[WedIndex + i])\n",
        "        numOfObjectsIntroduced.append(tempNumOfObjectsIntroduced[ThuIndex + i])\n",
        "        numOfObjectsIntroduced.append(tempNumOfObjectsIntroduced[FriIndex + i])\n",
        "        numOfObjectsIntroduced.append(tempNumOfObjectsIntroduced[SatIndex + i])\n",
        "        numOfObjectsIntroduced.append(tempNumOfObjectsIntroduced[SunIndex + i])\n",
        "\n",
        "    # interarrivalTime for objects introduction in a day\n",
        "    pareto_alpha_interArrival = 1.0073\n",
        "    numOfDays = len(numOfObjectsIntroduced)\n",
        "    for i in range(numOfDays):\n",
        "        objectsCountInDay = int(np.round(numOfObjectsIntroduced)[i])\n",
        "        if WITH_INTRODUCTION:\n",
        "            interArrivals.append(generateRandVariate('pareto', {'alpha': pareto_alpha_interArrival}, objectsCountInDay))\n",
        "        else:\n",
        "            interArrivals.append([0]*objectsCountInDay)\n",
        "    NUM_OF_OBJECTS = int(sum(np.round(numOfObjectsIntroduced)))\n",
        "\n",
        "\n",
        "def generateObjectIntroductionOrder():\n",
        "    return np.random.permutation(range(len(objectPopularities)))+1\n",
        "\n",
        "\n",
        "\"\"\"#########################  Object lifespan  ######################################################################\"\"\"\n",
        "def generateLifeSpans(numOfObjects, objMode):\n",
        "    logNormal_mu_mean = 3.0935\n",
        "    logNormal_mu_std = 0.9612\n",
        "    logNormal_sigma_mean = 1.1417\n",
        "    logNormal_sigma_std = 0.3067\n",
        "    pareto_alpha_mean = 1.7023\n",
        "    pareto_alpha_std = 0.2092\n",
        "    lifeSpans = []\n",
        "\n",
        "    logNormalMu = generateRandVariate('normal', {'mu': logNormal_mu_mean, 'sigma': logNormal_mu_std}, 1)[0]\n",
        "    logNormalSigma = generateRandVariate('normal', {'mu': logNormal_sigma_mean, 'sigma': logNormal_sigma_std}, 1)[0]\n",
        "\n",
        "    paretoAlpha = generateRandVariate('normal', {'mu': pareto_alpha_mean, 'sigma': pareto_alpha_std}, 1)[0]\n",
        "\n",
        "    for i in range(numOfObjects):\n",
        "        if objMode[i] is 'regular':\n",
        "            tmpLifeSpan = generateRandVariate('logNormal', {'mu': logNormalMu, 'sigma': logNormalSigma}, 1)[0]\n",
        "        elif objMode[i] is 'news':\n",
        "            tmpLifeSpan = generateRandVariate('pareto', {'alpha': paretoAlpha}, 1)[0]\n",
        "        if tmpLifeSpan > 80:\n",
        "            tmpLifeSpan = random.randint(2, 80)\n",
        "        lifeSpans.append((i+1, tmpLifeSpan))\n",
        "    return lifeSpans\n",
        "\n",
        "\n",
        "\"\"\"#########################  Object Generation  ####################################################################\"\"\"\n",
        "def normalizePopularities():\n",
        "    normalized = np.array(objectPopularities)/max(objectPopularities)\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def getBinInterval(time):\n",
        "    return (math.floor(time/float(3600)))/float(23)\n",
        "\n",
        "\n",
        "def generateObjects():\n",
        "    global ObjectsLifeSpan\n",
        "    global introductionOrder\n",
        "    global sortedOnIntoTime\n",
        "    global maxEndDay\n",
        "    normalizedPop = normalizePopularities()\n",
        "\n",
        "    for i in range(len(normalizedPop)):\n",
        "         lifeSpanType.append(getObjectType())\n",
        "    # tuple (objID, LifeSpan), objID from 1 to N\n",
        "    ObjectsLifeSpan = generateLifeSpans(len(objectPopularities), lifeSpanType)\n",
        "    introductionOrder = generateObjectIntroductionOrder()   # objectIntroductionOrder from 1 to N\n",
        "    for i in range(1, len(objectPopularities)+1):\n",
        "        requestGenInfo[i] = {'startDay': 0, 'lifeSpan': 0, 'endDay': 0, 'arrivalTime': 0, 'type': '', 'freq': 0,\n",
        "                             'unitPerDay': 0} # From 1 to N\n",
        "        startTimes[i] = 0\n",
        "\n",
        "    objCnt = 0\n",
        "    dayCnt = 0\n",
        "    for i in range(len(numOfObjectsIntroduced)):\n",
        "        dayTime = 0\n",
        "        dayCnt = dayCnt+round(dayGaps[i])\n",
        "        for j in range(int(np.round(numOfObjectsIntroduced)[i])):\n",
        "            objIntroduced = introductionOrder[objCnt]\n",
        "            dayTime = dayTime+interArrivals[i][j]\n",
        "            requestGenInfo[objIntroduced]['startDay'] = dayCnt\n",
        "            requestGenInfo[objIntroduced]['arrivalTime'] = dayTime\n",
        "            requestGenInfo[objIntroduced]['lifeSpan'] = ObjectsLifeSpan[objIntroduced-1][1]\n",
        "            requestGenInfo[objIntroduced]['type'] = lifeSpanType[objIntroduced-1]\n",
        "            requestGenInfo[objIntroduced]['freq'] = objectPopularities[objIntroduced-1]\n",
        "\n",
        "            # Generating at least a minimum number of requests per day\n",
        "            if requestGenInfo[objIntroduced]['freq'] / requestGenInfo[objIntroduced]['lifeSpan'] \\\n",
        "                    < MIN_REQ_PER_DAY_THRESHOLD:\n",
        "                # generate a random number for which number to update\n",
        "                decision = random.uniform(0, 1)\n",
        "                if decision <= 0.5:\n",
        "                    # update the object frequency\n",
        "                    life_span = random.randint(10, 80)\n",
        "                    requestGenInfo[objIntroduced]['freq'] = life_span * MIN_REQ_PER_DAY_THRESHOLD\n",
        "                    requestGenInfo[objIntroduced]['lifeSpan'] = life_span\n",
        "                else:\n",
        "                    # update the object life-span\n",
        "                    freq = random.randint(MIN_REQ_PER_DAY_THRESHOLD, 80*MIN_REQ_PER_DAY_THRESHOLD)\n",
        "                    requestGenInfo[objIntroduced]['freq'] = freq\n",
        "                    requestGenInfo[objIntroduced]['lifeSpan'] = freq / MIN_REQ_PER_DAY_THRESHOLD\n",
        "\n",
        "            startTimes[objIntroduced] = dayCnt+getBinInterval(dayTime)\n",
        "\n",
        "            requestGenInfo[objIntroduced]['endDay'] = requestGenInfo[objIntroduced]['lifeSpan'] + \\\n",
        "                                                      requestGenInfo[objIntroduced]['startDay']\n",
        "            requestGenInfo[objIntroduced]['totalDens'] = math.pow(requestGenInfo[objIntroduced]['lifeSpan'],\n",
        "                                                                  hourly_request_function_degree)\n",
        "\n",
        "            objectLengths.append([objIntroduced, requestGenInfo[objIntroduced]['startDay'],\n",
        "                                  requestGenInfo[objIntroduced]['lifeSpan'], requestGenInfo[objIntroduced]['endDay'],\n",
        "                                  requestGenInfo[objIntroduced]['freq']])\n",
        "\n",
        "            if requestGenInfo[objIntroduced]['endDay'] > maxEndDay:\n",
        "                maxEndDay = requestGenInfo[objIntroduced]['endDay']\n",
        "            objCnt = objCnt+1\n",
        "\n",
        "    sortedOnIntoTime = sorted(startTimes, key=startTimes.get)\n",
        "\n",
        "\n",
        "def generateDiurnalAccess(obj, diurnalRatio, dayCnt):\n",
        "    global requests\n",
        "\n",
        "    lifeTimeLeft = requestGenInfo[obj]['lifeSpan']\n",
        "\n",
        "    if lifeTimeLeft > 1:\n",
        "        lastDay = requestGenInfo[obj]['endDay']\n",
        "        objCount = abs(requestGenInfo[obj]['freq']*(((math.pow(dayCnt-lastDay, hourly_request_function_degree)\n",
        "                       - math.pow(lastDay-dayCnt+1, hourly_request_function_degree)))/requestGenInfo[obj]['totalDens']))\n",
        "        requestGenInfo[obj]['lifeSpan'] = requestGenInfo[obj]['lifeSpan']-1\n",
        "        for i in range(len(diurnalRatio)):\n",
        "            tmpCount = int(np.round(objCount*diurnalRatio[i]))\n",
        "            if tmpCount != 0:\n",
        "                tmpLambda = (tmpCount/float(3600))\n",
        "                reqInterArrivals = generateRandVariate('exp', {'mu': tmpLambda}, tmpCount)\n",
        "                for tmpInter in reqInterArrivals:\n",
        "                    requests.append((obj, (curTime[obj-1]+tmpInter)))\n",
        "                    curTime[obj-1] = curTime[obj-1]+tmpInter\n",
        "\n",
        "    else:\n",
        "        lastDay = requestGenInfo[obj]['endDay']\n",
        "        objCount = abs(requestGenInfo[obj]['freq']*(((math.pow(lastDay-dayCnt, hourly_request_function_degree)\n",
        "                       - math.pow(lastDay-(dayCnt+requestGenInfo[obj]['lifeSpan']), hourly_request_function_degree))) /\n",
        "                                                    requestGenInfo[obj]['totalDens']))\n",
        "        spanToGenerate = int(math.floor(requestGenInfo[obj]['lifeSpan']*10))\n",
        "        requestGenInfo[obj]['lifeSpan'] = 0\n",
        "\n",
        "        for i in range(spanToGenerate):\n",
        "            tmpCount = int(np.round(objCount*diurnalRatio[i]))\n",
        "            if tmpCount != 0:\n",
        "                tmpLambda = (tmpCount/float(3600))\n",
        "\n",
        "                reqInterArrivals = generateRandVariate('exp', {'mu': tmpLambda}, tmpCount)\n",
        "                for tmpInter in reqInterArrivals:\n",
        "                    requests.append((obj, (curTime[obj-1]+tmpInter)))\n",
        "                    curTime[obj-1] = curTime[obj-1]+tmpInter\n",
        "\n",
        "\n",
        "\"\"\"#########################  Requests Generation  ##################################################################\"\"\"\n",
        "def generateRequests():\n",
        "    global requests\n",
        "    global curTime\n",
        "\n",
        "    OUTPUTFILENAME = '{0}/mediSynDataset_x{1}_O{2}.csv'.format(OUTPUTDIR, hourly_request_function_degree, NUM_OF_OBJECTS)\n",
        "    if not os.path.isfile(OUTPUTFILENAME):\n",
        "        fi = open(OUTPUTFILENAME, 'w')\n",
        "        fi.write('object_ID,request_time\\n')\n",
        "        fi.close()\n",
        "\n",
        "    dayCount = requestGenInfo[sortedOnIntoTime[0]]['startDay']\n",
        "    reqGendf = pd.DataFrame.from_dict(requestGenInfo, orient='index')\n",
        "    reqGendf['objID'] = reqGendf.index\n",
        "\n",
        "    while dayCount <= maxEndDay:\n",
        "        objList = list(reqGendf[(reqGendf['startDay'] <= dayCount) & (reqGendf['endDay'] >= dayCount)]['objID'])\n",
        "        for obj in objList:\n",
        "            if curTime[obj-1] == 0:\n",
        "                curTime[obj-1] = (dayCount*86400) + requestGenInfo[obj]['arrivalTime']\n",
        "\n",
        "            generateDiurnalAccess(obj, lambdas, dayCount)\n",
        "\n",
        "        dayCount = dayCount + 1\n",
        "        if dayCount % 20 == 0:\n",
        "            requests = sorted(requests, key=lambda x: x[1])\n",
        "            saveRequestsToFile(OUTPUTFILENAME)\n",
        "            requests = []\n",
        "            print('{} Days Processed of {} Total Days'.format(dayCount, int(maxEndDay)))\n",
        "    print('MediSyn Dataset Saved to Output file: {}'.format(OUTPUTFILENAME))\n",
        "\n",
        "\n",
        "def saveRequestsToFile(OUTPUTFILENAME):\n",
        "    with open(OUTPUTFILENAME, 'a') as resultFile:\n",
        "        wr = csv.writer(resultFile, dialect='excel')\n",
        "        wr.writerows(requests)\n",
        "\n",
        "\n",
        "\"\"\"##################################################################################################################\"\"\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    initialize()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import ValuesView\n",
        "# LSTM 학습을 위한 입력 시퀀스 생성\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('Datasets/mediSynDataset_x2_O1034.csv') # 데이터 불러오기\n",
        "\n",
        "df['hour'] = df['request_time'] // 3600  # 초 단위 -> 시간 단위 버킷\n",
        "\n",
        "object_ids = df['object_ID'].unique()\n",
        "object_ids.sort()\n",
        "num_objects = len(object_ids)\n",
        "print(\"총 객체 수: \", num_objects)\n",
        "\n",
        "pivot = df.groupby(['hour', 'object_ID']).size().unstack(fill_value=0)\n",
        "pivot = pivot.reindex(columns=object_ids, fill_value=0) # 객체 ID 순서 맞추기\n",
        "\n",
        "# 확률 벡터로 정규화\n",
        "probs = pivot.div(pivot.sum(axis=1), axis=0).fillna(0)\n",
        "\n",
        "m, K = 20, 26  # 과거 20시간 -> 미래 10시간 예측한다고 가정\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(len(probs) - m - K):\n",
        "  x_seq = probs.iloc[i:i+m].values  # shape: (m, d)\n",
        "  x_next = probs.iloc[i+m:i+m+K].values\n",
        "  X.append(x_seq)\n",
        "  y.append(x_next)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"X.shape =\", X.shape)\n",
        "print(\"y.shape =\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8kpjD3VX0IX",
        "outputId": "f018cd0a-0056-4a3e-e3cb-3bceffdfa569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 객체 수:  1034\n",
            "X.shape = (4738, 20, 1034)\n",
            "y.shape = (4738, 26, 1034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 예측 모델\n",
        "# 과거 일정 시간 동안의 요청 분포를 받아서 앞으로의 일정 시간 동안의 요청 분포를 예측하는 구조\n",
        "# Encoder: 과거 m시간 동안의 요청 분포((m, 1033) 시퀀스)를 받아서, 마지막 hidden state와 cell state로 응축된 정보를 생성 -> context vector로 요약\n",
        "# Decoder: Encoder에서 받은 context vector를 바탕으로, 앞으로 k시간 동안 어떤 객체들이 얼마나 요청될지를 시퀀스 형태로 한 시간씩 예측\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, RepeatVector\n",
        "\n",
        "def build_seq2seq_model(m, K, num_objects):\n",
        "  # ----- Encoder -----\n",
        "  encoder_inputs = Input(shape=(m, num_objects)) # (batch, m(시간), num_objects(객체수))\n",
        "  encoder_lstm = LSTM(128, return_state=True) # LSTM이 마지막 시점의 hidden state와 cell state를 반환\n",
        "  encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs) # state_h와 state_c는 Decoder의 초기 상태로 사용됨\n",
        "  encoder_states = [state_h, state_c] # Encoder의 상태를 Decoder에 전달하기 위해 리스트로 묶음\n",
        "\n",
        "  # ----- Decoder -----\n",
        "  decoder_inputs = RepeatVector(K)(encoder_outputs) # (batch, K, 128): Encoder의 마지막 출력을 K번 복제해서 Decoder 입력으로 사용, Decoder는 K시간 동안 예측을 수행\n",
        "  decoder_lstm = LSTM(128, return_sequences=True) # LSTM의 hidden size는 64, return_sequences=True는 각 시점마다 출력을 반환\n",
        "  decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "  # 객체별 확률 예측 (각 시점당 num_objects 개 출력)\n",
        "  decoder_dense = TimeDistributed(Dense(num_objects, activation='softmax')) # softmax는 확률 분포로 만들어줌\n",
        "  output_seq = decoder_dense(decoder_outputs)\n",
        "\n",
        "  # ----- 모델 구성 -----\n",
        "  model = Model(encoder_inputs, output_seq)\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "  return model\n",
        "\n",
        "\n",
        "num_objects = 1034  # 객체 수\n",
        "m = 20              # 과거 시간: 입력 시퀀스 길이\n",
        "K = 26             # 미래 시간: 출력 시퀀스 길이\n",
        "\n",
        "model = build_seq2seq_model(m, K, num_objects)\n",
        "\n",
        "# 데이터: X shape = (samples, 20, 50), y shape = (samples, 10, 50)\n",
        "# history = model.fit(X, y, epochs=30, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# 학습 후 추론 코드\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 시퀀스 데이터를 학습/검증/테스트로 나누기\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 학습 시에도 X_train, y_train 사용\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# 추론 (예측)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"예측 결과 형태:\", y_pred.shape)\n",
        "print(\"y_test.shape =\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlE9ufUZhl7N",
        "outputId": "1ff8f573-6a15-4f5d-c2b5-e1f939b8b193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - loss: 2.0000e-05 - mae: 0.0016 - val_loss: 1.7732e-05 - val_mae: 0.0016\n",
            "Epoch 2/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 1.8906e-05 - mae: 0.0016 - val_loss: 1.7730e-05 - val_mae: 0.0016\n",
            "Epoch 3/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.9470e-05 - mae: 0.0016 - val_loss: 1.7727e-05 - val_mae: 0.0016\n",
            "Epoch 4/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1.9560e-05 - mae: 0.0016 - val_loss: 1.7724e-05 - val_mae: 0.0016\n",
            "Epoch 5/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.9547e-05 - mae: 0.0016 - val_loss: 1.7720e-05 - val_mae: 0.0016\n",
            "Epoch 6/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 1.8600e-05 - mae: 0.0016 - val_loss: 1.7716e-05 - val_mae: 0.0016\n",
            "Epoch 7/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.9312e-05 - mae: 0.0016 - val_loss: 1.7711e-05 - val_mae: 0.0016\n",
            "Epoch 8/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.9672e-05 - mae: 0.0016 - val_loss: 1.7706e-05 - val_mae: 0.0016\n",
            "Epoch 9/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 1.9899e-05 - mae: 0.0016 - val_loss: 1.7700e-05 - val_mae: 0.0016\n",
            "Epoch 10/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 1.9615e-05 - mae: 0.0016 - val_loss: 1.7694e-05 - val_mae: 0.0016\n",
            "Epoch 11/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 1.9912e-05 - mae: 0.0016 - val_loss: 1.7687e-05 - val_mae: 0.0016\n",
            "Epoch 12/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1.9506e-05 - mae: 0.0016 - val_loss: 1.7678e-05 - val_mae: 0.0016\n",
            "Epoch 13/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 1.9393e-05 - mae: 0.0016 - val_loss: 1.7669e-05 - val_mae: 0.0016\n",
            "Epoch 14/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.9055e-05 - mae: 0.0016 - val_loss: 1.7656e-05 - val_mae: 0.0016\n",
            "Epoch 15/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 1.8812e-05 - mae: 0.0016 - val_loss: 1.7640e-05 - val_mae: 0.0016\n",
            "Epoch 16/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.9469e-05 - mae: 0.0016 - val_loss: 1.7616e-05 - val_mae: 0.0016\n",
            "Epoch 17/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.8876e-05 - mae: 0.0016 - val_loss: 1.7577e-05 - val_mae: 0.0016\n",
            "Epoch 18/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.9086e-05 - mae: 0.0016 - val_loss: 1.7507e-05 - val_mae: 0.0016\n",
            "Epoch 19/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 1.8870e-05 - mae: 0.0016 - val_loss: 1.7352e-05 - val_mae: 0.0016\n",
            "Epoch 20/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 1.8831e-05 - mae: 0.0016 - val_loss: 1.7080e-05 - val_mae: 0.0016\n",
            "Epoch 21/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1.8543e-05 - mae: 0.0016 - val_loss: 1.6836e-05 - val_mae: 0.0015\n",
            "Epoch 22/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 1.8621e-05 - mae: 0.0015 - val_loss: 1.6732e-05 - val_mae: 0.0015\n",
            "Epoch 23/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 1.7981e-05 - mae: 0.0015 - val_loss: 1.6679e-05 - val_mae: 0.0015\n",
            "Epoch 24/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 1.8315e-05 - mae: 0.0015 - val_loss: 1.6643e-05 - val_mae: 0.0015\n",
            "Epoch 25/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 1.8745e-05 - mae: 0.0015 - val_loss: 1.6613e-05 - val_mae: 0.0015\n",
            "Epoch 26/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.8705e-05 - mae: 0.0015 - val_loss: 1.6611e-05 - val_mae: 0.0015\n",
            "Epoch 27/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.8376e-05 - mae: 0.0015 - val_loss: 1.6589e-05 - val_mae: 0.0015\n",
            "Epoch 28/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 1.8574e-05 - mae: 0.0015 - val_loss: 1.6563e-05 - val_mae: 0.0015\n",
            "Epoch 29/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 1.8251e-05 - mae: 0.0015 - val_loss: 1.6544e-05 - val_mae: 0.0015\n",
            "Epoch 30/30\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.8022e-05 - mae: 0.0015 - val_loss: 1.6530e-05 - val_mae: 0.0015\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "예측 결과 형태: (948, 26, 1034)\n",
            "y_test.shape = (948, 26, 1034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 캐시 정책 구현\n",
        "# 예측 결과를 기반으로 DeepCache의 캐싱 정책을 구현하고 이를 통해 기본 캐싱 전략과 비교평가\n",
        "M = 10 # 캐시에 넣을 상위 M개 객체\n",
        "top_objects_each_t = []\n",
        "\n",
        "# 예측 결과에서 Top-M 객체 추출\n",
        "for i in range(len(y_pred)):\n",
        "    for t in range(K - 1):    # t+1 예측을 위해 K-1까지만\n",
        "        next_probs = y_pred[i, t+1]  # 다음 시점 확률 분포\n",
        "        top_indices = next_probs.argsort()[-M:][::-1]  # 상위 M개 객체 인덱스\n",
        "        top_objects_each_t.append(top_indices)\n",
        "\n",
        "# 실제 요청 로그 준비\n",
        "actual_requests = df.sort_values('request_time')['object_ID'].tolist()"
      ],
      "metadata": {
        "id": "-zvVXjC0UR_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 캐시 시뮬레이션 (LRU + 예측 기반) -> 성능 평가\n",
        "from collections import deque\n",
        "\n",
        "def simulate_deepcache(actual_requests, top_objects_each_t, cache_size=150, insert_interval=20):\n",
        "  \"\"\"\n",
        "    DeepCache 캐시 시뮬레이션을 수행하고 hit ratio를 반환합니다.\n",
        "\n",
        "    Parameters:\n",
        "        actual_requests (list): 실제 요청 객체 ID 리스트\n",
        "        top_objects_each_t (list of lists): 예측된 상위 M개의 객체 리스트 (매 insert_interval마다 1개 list)\n",
        "        cache_size (int): 캐시 크기\n",
        "        insert_interval (int): 예측 객체를 캐시에 넣는 주기 (예: 100 step마다 예측 사용)\n",
        "\n",
        "    Returns:\n",
        "        hit_ratio (float): 캐시 hit 비율\n",
        "  \"\"\"\n",
        "\n",
        "  cache = deque(maxlen=cache_size)\n",
        "  hit = 0\n",
        "  total = 0\n",
        "  fake_insert_idx = 0\n",
        "\n",
        "  for t, req in enumerate(actual_requests):\n",
        "    # 예측한 객체를 먼저 캐시에 넣기\n",
        "    if t % insert_interval == 0 and fake_insert_idx < len(top_objects_each_t):\n",
        "      fake_objs = top_objects_each_t[fake_insert_idx]\n",
        "      for obj in fake_objs:\n",
        "        if obj not in cache:\n",
        "          cache.append(obj)\n",
        "      fake_insert_idx += 1\n",
        "\n",
        "    total += 1\n",
        "    if req in cache:\n",
        "      hit += 1\n",
        "    else:\n",
        "      cache.append(req)\n",
        "\n",
        "  hit_ratio = hit / total if total > 0 else 0\n",
        "  return hit_ratio\n",
        "\n",
        "deepcache_hit = simulate_deepcache(actual_requests, top_objects_each_t, cache_size=150, insert_interval=20)\n",
        "print(f\"DeepCache 기반 캐시 hit ratio: {deepcache_hit:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVR8Iiupi9Mb",
        "outputId": "4643b687-cad0-4b71-d083-608747fc95a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepCache 기반 캐시 hit ratio: 0.7779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 LRU 캐시와 성능 비교\n",
        "# LRU 캐시 시뮬레이터 코드\n",
        "from collections import OrderedDict\n",
        "\n",
        "class LRUCache:\n",
        "  def __init__(self, capacity):\n",
        "    self.capacity = capacity\n",
        "    self.cache = OrderedDict()\n",
        "    self.hit = 0\n",
        "    self.miss = 0\n",
        "\n",
        "  def request(self, obj_id):\n",
        "    if obj_id in self.cache:\n",
        "      self.cache.move_to_end(obj_id)\n",
        "      self.hit += 1\n",
        "    else:\n",
        "      self.miss += 1\n",
        "      if len(self.cache) >= self.capacity:\n",
        "        self.cache.popitem(last=False) # 길이를 넘으면 pop\n",
        "      self.cache[obj_id] = True # 최근 것 push\n",
        "\n",
        "  def get_hit_ratio(self):\n",
        "    total = self.hit + self.miss\n",
        "    return self.hit / total if total > 0 else 0"
      ],
      "metadata": {
        "id": "JPUZgmb0UvjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 요청 로그를 읽어서 LRU 시뮬레이션 수행\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Datasets/mediSynDataset_x2_O1034.csv')\n",
        "requests = df['object_ID'].tolist()\n",
        "\n",
        "cache_size = 5\n",
        "lru = LRUCache(cache_size)\n",
        "\n",
        "for obj_id in requests:\n",
        "  lru.request(obj_id)\n",
        "\n",
        "print(\"LRU 캐시 hit ratio:\", round(lru.get_hit_ratio(), 4))"
      ],
      "metadata": {
        "id": "HG2f4678Wl4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 비교 결과 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ratios = [\n",
        "    simulate_deepcache(actual_requests, top_objects_each_t, cache_size=150, insert_interval=20),  # DeepCache\n",
        "    lru.get_hit_ratio()  # 기존 LRU 객체에서 hit ratio 가져오기\n",
        "]\n",
        "labels = ['DeepCache', 'LRU']\n",
        "\n",
        "plt.bar(labels, ratios, color=['skyblue', 'salmon'])\n",
        "plt.ylabel('Hit Ratio')\n",
        "plt.title('DeepCache vs LRU Performance')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "LuskNjrYW4Ud",
        "outputId": "2180c56e-8b19-472c-f636-cbd14bad3ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'simulate_deepcache' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-31d260e8402d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ratios = [\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msimulate_deepcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_requests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_objects_each_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# DeepCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hit_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 기존 LRU 객체에서 hit ratio 가져오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'simulate_deepcache' is not defined"
          ]
        }
      ]
    }
  ]
}