{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyODoV2IsWNRzy5e/485WHon"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11996326,"sourceType":"datasetVersion","datasetId":7515257}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Cache with SyntheticDataset\n\nReference Paper\n> Deep Cache: A Deep Learning Based Framework For Content Caching","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:53:36.857869Z","iopub.execute_input":"2025-05-29T12:53:36.858127Z","iopub.status.idle":"2025-05-29T12:53:38.833346Z","shell.execute_reply.started":"2025-05-29T12:53:36.858106Z","shell.execute_reply":"2025-05-29T12:53:38.832694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use GPU if available, otherwise fall back to CPU\nimport tensorflow as tf\n\ndevice = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:53:38.834388Z","iopub.execute_input":"2025-05-29T12:53:38.834695Z","iopub.status.idle":"2025-05-29T12:53:53.782239Z","shell.execute_reply.started":"2025-05-29T12:53:38.834677Z","shell.execute_reply":"2025-05-29T12:53:53.781516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\ndf = pd.read_csv('/kaggle/input/syntheticDataset_O50.csv') \ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:53:53.782988Z","iopub.execute_input":"2025-05-29T12:53:53.783464Z","iopub.status.idle":"2025-05-29T12:53:53.959792Z","shell.execute_reply.started":"2025-05-29T12:53:53.783445Z","shell.execute_reply":"2025-05-29T12:53:53.959085Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split Data \n- To align with the paper's goal which is predicting the future characteristics of an object based on past logs, we split the dataset into 60% for training and 40% for evaluation.","metadata":{}},{"cell_type":"code","source":"cut_idx = int(len(df) * 0.6)\ntrain_df = df.iloc[:cut_idx].reset_index(drop=True)\ntest_df = df.iloc[cut_idx:].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:53:53.960528Z","iopub.execute_input":"2025-05-29T12:53:53.960742Z","iopub.status.idle":"2025-05-29T12:53:53.966021Z","shell.execute_reply.started":"2025-05-29T12:53:53.960723Z","shell.execute_reply":"2025-05-29T12:53:53.965286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering\n> \"For dataset 1, the probability of object $o^i$ is calculated as $Ni$ /1000, where $N^i$ represents the number of occurrences of $o^i$ in the window of past 1K objects.\"\n- Using step=1 generated too much data, so we used step=100 to make it manageable.","metadata":{}},{"cell_type":"code","source":"window_size = 1000\nstep = 100\nm, k = 20, 10 # Sequence length for input(m) and output(k)\n\nX = []\ny = []\n\nobject_ids = df['object_ID'].unique()\nobject_ids.sort()\nnum_objects = len(object_ids) # Number of unique objects: 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:53:53.968076Z","iopub.execute_input":"2025-05-29T12:53:53.968294Z","iopub.status.idle":"2025-05-29T12:53:53.987584Z","shell.execute_reply.started":"2025-05-29T12:53:53.968277Z","shell.execute_reply":"2025-05-29T12:53:53.987076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate training data for sequence-to-sequence modeling from object request logs\nfor i in range(0, len(train_df) - window_size * (m + k), step):\n    seq = train_df['object_ID'].iloc[i : i + window_size * (m + k)]\n    x_seq, y_seq = [], []\n    \n    # Build the input sequence: m windows of past requests\n    for j in range(m):\n        window = seq[j * window_size : (j + 1) * window_size]\n        counts = window.value_counts(normalize=True).reindex(object_ids, fill_value=0).values\n        x_seq.append(counts)\n        \n    # Build the output sequence: k windows of future requests\n    for j in range(k):\n        window = seq[(m + j) * window_size : (m + j + 1) * window_size]\n        counts = window.value_counts(normalize=True).reindex(object_ids, fill_value=0).values\n        y_seq.append(counts)\n\n    X.append(x_seq) # (#samples, 20, d)\n    y.append(y_seq) # (#samples, 26, d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:53:53.988337Z","iopub.execute_input":"2025-05-29T12:53:53.988597Z","iopub.status.idle":"2025-05-29T12:54:11.710590Z","shell.execute_reply.started":"2025-05-29T12:53:53.988574Z","shell.execute_reply":"2025-05-29T12:54:11.710037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = np.array(X)\ny = np.array(y)\n\n# Reshape X and y to fit LSTM input requirements:\n# X: (samples * num_objects, m, 1)\n# y: (samples * num_objects, K, 1)\nX = X.transpose(0, 2, 1).reshape(-1, m, 1)  \ny = y.transpose(0, 2, 1).reshape(-1, k, 1)  \nprint(\"X.shape (reshaped):\", X.shape)\nprint(\"y.shape (reshaped):\", y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:54:11.711383Z","iopub.execute_input":"2025-05-29T12:54:11.711675Z","iopub.status.idle":"2025-05-29T12:54:11.768575Z","shell.execute_reply.started":"2025-05-29T12:54:11.711644Z","shell.execute_reply":"2025-05-29T12:54:11.767791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save(\"X_dataset1_window.npy\", X)\nnp.save(\"y_dataset1_window.npy\", y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:54:11.769385Z","iopub.execute_input":"2025-05-29T12:54:11.769692Z","iopub.status.idle":"2025-05-29T12:54:11.786900Z","shell.execute_reply.started":"2025-05-29T12:54:11.769666Z","shell.execute_reply":"2025-05-29T12:54:11.786274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build LSTM Model\n> \"For our datasets, we use a two-layer depth LSTM Encoder-Decoder model with 128 and 64 as the number of hidden units. ... The loss function is chosen as mean-squared-error (MSE).\"","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, RepeatVector\nfrom tensorflow.keras.models import Model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:54:11.787506Z","iopub.execute_input":"2025-05-29T12:54:11.787689Z","iopub.status.idle":"2025-05-29T12:54:11.854166Z","shell.execute_reply.started":"2025-05-29T12:54:11.787673Z","shell.execute_reply":"2025-05-29T12:54:11.853609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_seq2seq_model(m, k):\n    # ----- Encoder -----\n    encoder_inputs = Input(shape=(m, 1))  # Input shape: (sequence_length, 1)\n\n    # Two-layer encoder with 128 and 64 hidden units\n    encoder_lstm_1 = LSTM(128, return_sequences=True)\n    encoder_lstm_2 = LSTM(64, return_state=True)\n\n    x = encoder_lstm_1(encoder_inputs)\n    encoder_outputs, state_h, state_c = encoder_lstm_2(x)\n    encoder_states = [state_h, state_c]  # Final encoder states passed to decoder\n\n    # ----- Decoder -----\n    decoder_inputs = RepeatVector(k)(encoder_outputs)  # Repeat context vector for k time steps\n\n    # Two-layer decoder with 128 and 64 hidden units\n    decoder_lstm_1 = LSTM(128, return_sequences=True)\n    decoder_lstm_2 = LSTM(64, return_sequences=True)\n\n    x = decoder_lstm_1(decoder_inputs, initial_state=encoder_states)\n    decoder_outputs = decoder_lstm_2(x)\n\n    # Predict one value per time step\n    decoder_dense = TimeDistributed(Dense(1))\n    output_seq = decoder_dense(decoder_outputs)\n\n    # Compile the model with MSE loss and MAE metric\n    model = Model(encoder_inputs, output_seq)\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:54:11.854835Z","iopub.execute_input":"2025-05-29T12:54:11.855010Z","iopub.status.idle":"2025-05-29T12:54:11.860338Z","shell.execute_reply.started":"2025-05-29T12:54:11.854997Z","shell.execute_reply":"2025-05-29T12:54:11.859607Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train Model\n> \"We ran our experiments for a number of epochs equal to 30, with the batch size set to 10% of the training data.\"","metadata":{}},{"cell_type":"code","source":"batch_size = int(len(X) * 0.1)\nprint(\"Batch size:\", batch_size)\n\nmodel = build_seq2seq_model(m, k)\nmodel.fit(X, y, epochs=30, batch_size=batch_size)\n\n# Predict\ny_pred = model.predict(X)  \n\nprint(\"Prediction Shape:\", y_pred.shape)","metadata":{"id":"OWBu2M4GXrZv","outputId":"716eddaa-3303-418c-923d-3d669cc313b9","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:54:11.861110Z","iopub.execute_input":"2025-05-29T12:54:11.861339Z","iopub.status.idle":"2025-05-29T12:55:21.027342Z","shell.execute_reply.started":"2025-05-29T12:54:11.861324Z","shell.execute_reply":"2025-05-29T12:55:21.026552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cache Policy Setting\nForeveryobjectrequestoit attimet,wegenerateavaryingnumber of “fake object requests” (denoted as Ft ). For dataset 1, we generate Ft by calculating the top M = 5 objects with highest probability at t + 1. ","metadata":{}},{"cell_type":"code","source":"M = 5\ntop_objects_each_t = []\nfor i in range(len(y_pred)):\n    next_probs = y_pred[i, 0].flatten()\n    top_indices = next_probs.argsort()[-M:][::-1]\n    top_objects_each_t.append(list(top_indices))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:55:23.814837Z","iopub.execute_input":"2025-05-29T12:55:23.815328Z","iopub.status.idle":"2025-05-29T12:55:24.016517Z","shell.execute_reply.started":"2025-05-29T12:55:23.815303Z","shell.execute_reply":"2025-05-29T12:55:24.015711Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Integral Operator\n> \"The operator is a simple merge operator, where the actual object request is followed by all the fake requests generated by our Caching Policy. This helps us to update the state of the cache by prefetching objects based on future object popularity and evict unpopular ones.\"","metadata":{}},{"cell_type":"code","source":"# Merge real requests and fake requests\nmerged_requests = []\nfake_insert_idx = 0\ninsert_interval = 5\n\nactual_requests = test_df.sort_values('request_time')['object_ID'].tolist()\n\nfor t, req in enumerate(actual_requests):\n    merged_requests.append(req)\n    if fake_insert_idx < len(top_objects_each_t) and t % insert_interval == 0:\n        fake_objs = top_objects_each_t[fake_insert_idx]\n        merged_requests.extend(fake_objs)\n        fake_insert_idx += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:56:54.775076Z","iopub.execute_input":"2025-05-29T12:56:54.775814Z","iopub.status.idle":"2025-05-29T12:56:54.820074Z","shell.execute_reply.started":"2025-05-29T12:56:54.775780Z","shell.execute_reply":"2025-05-29T12:56:54.819514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LRU Cache simulator\nfrom collections import OrderedDict\n\nclass LRUCache:\n  def __init__(self, capacity):\n    self.capacity = capacity\n    self.cache = OrderedDict()\n    self.hit = 0\n    self.miss = 0\n\n  def request(self, obj_id):\n    if obj_id in self.cache:\n        self.cache.move_to_end(obj_id)\n        self.hit += 1\n        return 1\n    else:\n        self.miss += 1\n        if len(self.cache) >= self.capacity:\n            self.cache.popitem(last=False) \n        self.cache[obj_id] = True\n        return 0\n\n  def get_hit_ratio(self):\n    total = self.hit + self.miss\n    return self.hit / total if total > 0 else 0","metadata":{"id":"wDg2rcihoHav","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:55:28.473506Z","iopub.execute_input":"2025-05-29T12:55:28.474001Z","iopub.status.idle":"2025-05-29T12:55:28.479476Z","shell.execute_reply.started":"2025-05-29T12:55:28.473977Z","shell.execute_reply":"2025-05-29T12:55:28.478802Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Deep Cache vs Traditional LRU\n> \"We compare traditional LRU with Deep Cache, and without Deep Cache.\"\n> \"For dataset 1, we set the cache size to 5.\"\n- When the cache size is small (cache size = 5), incorrect prefetching by DeepCache leads to cache pollution, evicting useful objects and resulting in a lower hit ratio compared to traditional LRU.\n- However, as the cache size increases, the impact of incorrect predictions diminishes and the benefit of correctly prefetched objects increases, allowing DeepCache to outperform traditional LRU.\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndeep_hit_ratios = []\ntrad_hit_ratios = []\ncache_sizes = list(range(5, 11))  # 5 to 10\n\nfor cache_size in cache_sizes:\n    # Execute LRU with Deep Cache (on merged requests)\n    lru_deep = LRUCache(cache_size)\n    hits_deep = []\n    for obj_id in merged_requests:\n        hit = lru_deep.request(obj_id)\n        hits_deep.append(hit)\n    deep_hit_ratios.append(round(lru_deep.get_hit_ratio(), 4))\n\n    # Execute traditional LRU (on raw requests)\n    lru_traditional = LRUCache(cache_size)\n    hits_trad = []\n    for obj_id in actual_requests:\n        hit = lru_traditional.request(obj_id)\n        hits_trad.append(hit)\n    trad_hit_ratios.append(round(lru_traditional.get_hit_ratio(), 4))\n\n# Calculate the absolute differences between DeepCache and Traditional LRU hit ratios\ndiffs = [abs(d - t) for d, t in zip(deep_hit_ratios, trad_hit_ratios)]\nmax_diff_idx = diffs.index(max(diffs)) # Find the index where the difference is the largest\n\ndeep_best_hit_ratio = deep_hit_ratios[max_diff_idx]\nlru_best_hit_ratio = trad_hit_ratios[max_diff_idx]\n\nprint(f\"Max difference at cache size {cache_sizes[max_diff_idx]}:\")\nprint(f\"DeepCache hit ratio: {deep_best_hit_ratio}\")\nprint(f\"Traditional LRU hit ratio: {lru_best_hit_ratio}\")\n\n\n# Visualize the result\nplt.figure(figsize=(8, 5))\nplt.plot(cache_sizes, deep_hit_ratios, marker='o', label='DeepCache (LSTM + LRU)')\nplt.plot(cache_sizes, trad_hit_ratios, marker='s', label='Traditional LRU')\nplt.xlabel('Cache Size')\nplt.ylabel('Hit Ratio')\nplt.title('Cache Hit Ratio vs Cache Size')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T13:07:43.399201Z","iopub.execute_input":"2025-05-29T13:07:43.399947Z","iopub.status.idle":"2025-05-29T13:07:44.170742Z","shell.execute_reply.started":"2025-05-29T13:07:43.399923Z","shell.execute_reply":"2025-05-29T13:07:44.169950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize the result","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nratios = [\n    deep_best_hit_ratio,\n    lru_best_hit_ratio\n]\nlabels = ['DeepCache', 'LRU']\n\nplt.bar(labels, ratios, color=['skyblue', 'salmon'])\nplt.ylabel('Hit Ratio')\nplt.title('DeepCache vs LRU Performance')\nplt.ylim(0, 1)\nplt.show()","metadata":{"id":"UR8p2f5Ofmi9","outputId":"db6f610b-5b21-41d8-e823-a24e9451df05","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T13:09:36.514039Z","iopub.execute_input":"2025-05-29T13:09:36.514767Z","iopub.status.idle":"2025-05-29T13:09:36.645743Z","shell.execute_reply.started":"2025-05-29T13:09:36.514744Z","shell.execute_reply":"2025-05-29T13:09:36.644987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hits_deep = np.array(hits_deep, dtype=int)\nhits_trad = np.array(hits_trad, dtype=int)\n\nmin_len = min(len(hits_trad), len(hits_deep))\n\nhits_trad = hits_trad[:min_len]\nhits_deep = hits_deep[:min_len]\nx = np.arange(1, min_len + 1)\n\nx = np.arange(1, len(hits_trad) + 1)\ncumulative_deep = np.cumsum(hits_deep)\ncumulative_trad = np.cumsum(hits_trad)\n\nstep = 10\nx = np.arange(1, min_len + 1)[::step]\ncumulative_trad = cumulative_trad[::step]\ncumulative_deep = cumulative_deep[::step]\n\ny_max = max(cumulative_deep.max(), cumulative_trad.max())\nyticks = np.arange(0, y_max + 1, 80000) \n\nplt.figure(figsize=(10, 6))\nplt.plot(x, cumulative_trad, label='LRU', color='red', linewidth=2)\nplt.plot(x, cumulative_deep, label='DeepCache (LSTM + LRU)', color='green', linewidth=2)\nplt.xlabel('Request Sequence', fontsize=12)\nplt.ylabel('Cumulative Hits', fontsize=12)\nplt.title('Cumulative Cache Hits Over Time', fontsize=14)\nplt.yticks(yticks, fontsize=10)  \nplt.xticks(fontsize=10)\nplt.grid(True)\nplt.legend(fontsize=12)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T13:09:38.969512Z","iopub.execute_input":"2025-05-29T13:09:38.970230Z","iopub.status.idle":"2025-05-29T13:09:39.170502Z","shell.execute_reply.started":"2025-05-29T13:09:38.970208Z","shell.execute_reply":"2025-05-29T13:09:39.169695Z"}},"outputs":[],"execution_count":null}]}